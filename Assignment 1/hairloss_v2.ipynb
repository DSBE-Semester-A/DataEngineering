{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611aeefc-f5ce-40e8-acc7-d04ac84da39a",
   "metadata": {},
   "source": [
    "## Some of the source codes are based on\n",
    "https://towardsdatascience.com/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in /home/jupyter/.local/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.9.0.tar.gz (595 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.6/595.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-pipeline-components>2 in /home/jupyter/.local/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.70.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp>2) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.14.0)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp>2)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp>2) (4.25.5)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.26.20)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>2) (3.1.4)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp>2) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.0.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.9.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp>2) (1.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2) (3.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2024.8.30)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (74.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (2.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp>2) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google-cloud-aiplatform==1.70.0\n",
      "google_cloud_pipeline_components version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict, List\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"endless-mile-435507-h9\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"europe-west1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://hairloss-de-temp\"   # e.g., gs://temp_de2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bace39-57ba-49ee-bd74-9eaf4093f471",
   "metadata": {},
   "source": [
    "#### Create Pipeline Components\n",
    "\n",
    "We can create a component from Python functions (inline) and from a container. We will first try inline python functions. \n",
    "Refer to  https://www.kubeflow.org/docs/components/pipelines/v2/components/lightweight-python-components/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1edd3e-d9cb-4fdd-a805-5404ec52aae8",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d1e7866-b653-44f6-8c74-84d41a767943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pipeline Component : Training DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_dt(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a DecisionTreeClassifier with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    data = pd.read_csv(features.path + \".csv\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.drop('Hair Loss',axis=1), \n",
    "                                                    data['Hair Loss'], test_size=0.30, \n",
    "                                                    random_state=101)\n",
    "    model_dt = DecisionTreeClassifier()\n",
    "    model_dt.fit(x_train, y_train)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"accuracy\": model_dt.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"dt\"\n",
    "   # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as model_file:\n",
    "        pickle.dump(model_dt, model_file)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe9845-3e68-4481-986f-3e1597c0b593",
   "metadata": {},
   "source": [
    "### Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45346bf0-12e4-4e05-aabd-f28ca2d3ac66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    data = pd.read_csv(features.path + \".csv\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.drop('Hair Loss',axis=1), \n",
    "                                                    data['Hair Loss'], test_size=0.30, \n",
    "                                                    random_state=101)\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(x_train, y_train)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"accuracy\": model_lr.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"lr\"\n",
    "   # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as model_file:\n",
    "        pickle.dump(model_lr, model_file)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dba522-a413-44d8-ad6d-aac136227f4f",
   "metadata": {},
   "source": [
    "### Prediction: DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41e6c669-e0f6-4e9f-bd01-870c41e45244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_dt(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    '''train a DecisionTreeClassifier with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    data = pd.read_csv(features.path + \".csv\")\n",
    "    xNew = data.loc[:, ['Genetics', 'Hormonal Changes', 'Medical Conditions', 'Medications & Treatments',\n",
    "               'Nutritional Deficiencies', 'Stress', 'Age', 'Poor Hair Care Habits',\n",
    "               'Environmental Factors', 'Smoking', 'Weight Loss']].values\n",
    "    #load the model\n",
    "    filename = model.path + \".pkl\"\n",
    "        \n",
    "    #Loading the saved model\n",
    "    model_dt = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    dfcp = data.copy()\n",
    "    result = model_dt.predict(xNew)   \n",
    "    y_classes = result.argmax(axis=-1)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pHairloss'] = y_classes.tolist()\n",
    "    dfcp.to_csv(results.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef60a1-c14b-4876-a120-44e52103a1ee",
   "metadata": {},
   "source": [
    "### Prediction LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "267f5b5a-0148-4081-9cf2-bfd30ed37580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_lr(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    import pickle  \n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    data = pd.read_csv(features.path + \".csv\")\n",
    "    xNew = data.loc[:, ['Genetics', 'Hormonal Changes', 'Medical Conditions', 'Medications & Treatments',\n",
    "               'Nutritional Deficiencies', 'Stress', 'Age', 'Poor Hair Care Habits',\n",
    "               'Environmental Factors', 'Smoking', 'Weight Loss']].values\n",
    "    #load the model\n",
    "    filename = model.path + \".pkl\"\n",
    "        \n",
    "    #Loading the saved model\n",
    "    model_lr = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    dfcp = data.copy()\n",
    "    result = model_lr.predict(xNew)   \n",
    "    y_classes = result.argmax(axis=-1)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pHairloss'] = y_classes.tolist()\n",
    "    dfcp.to_csv(results.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb3b13-e53c-4706-8c21-05a16d5502fb",
   "metadata": {},
   "source": [
    "### Algorithm Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e611b501-fa00-4eab-a179-0336adaaca38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(dt_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(dt_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if dt_metrics.get(\"accuracy\") > lr_metrics.get(\"accuracy\"):\n",
    "        return \"DT\"\n",
    "    else :\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60055440-2087-40a3-8cf6-517220014e69",
   "metadata": {},
   "source": [
    "### Upload model and metrics to google Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a85a2be2-a0ac-4571-99af-0ac2bc8d0dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(str(model.metadata[\"algo\"]) + '_model' + str(model.metadata[\"file_type\"])) \n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))       \n",
    "    \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c84923d8-2952-4ea0-8118-dc0ce7822f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @dsl.component(\n",
    "#     packages_to_install=[\"google-cloud-storage\"],\n",
    "#     base_image=\"python:3.10.7-slim\"\n",
    "# )\n",
    "# def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "#     '''upload model to gsc'''\n",
    "#     from google.cloud import storage   \n",
    "#     import logging \n",
    "#     import sys\n",
    "    \n",
    "#     logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "#     # upload the model to GCS\n",
    "#     client = storage.Client(project=project_id)\n",
    "#     bucket = client.bucket(model_repo)\n",
    "#     blob = bucket.blob('model.pkl')\n",
    "#     source_file_name= model.path + '.pkl'\n",
    "   \n",
    "#     blob.upload_from_filename(source_file_name)    \n",
    "    \n",
    "#     print(f\"File {source_file_name} uploaded to {model_repo}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a7d00-a9b1-4b09-91d7-bab1e103263a",
   "metadata": {},
   "source": [
    "### Trigger another CI-CD Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "411cc985-bf19-4ea4-b533-94b6150eb958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def run_build_trigger(project_id:str, trigger_id:str):\n",
    "    import sys\n",
    "    from google.cloud.devtools import cloudbuild_v1    \n",
    "    import logging \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Create a client\n",
    "    client = cloudbuild_v1.CloudBuildClient()\n",
    "    name = f\"projects/{project_id}/locations/europe-west1/triggers/{trigger_id}\"\n",
    "    # Initialize request argument(s)\n",
    "    request = cloudbuild_v1.RunBuildTriggerRequest(        \n",
    "        project_id=project_id,\n",
    "        trigger_id=trigger_id,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.run_build_trigger(request=request)\n",
    "    \n",
    "    logging.info(\"Trigger the CI-CD Pipeline: \" + trigger_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce80fda-8515-4f8b-afe0-459aa1f4fede",
   "metadata": {},
   "source": [
    "### Deploy the model at Vertex AI \n",
    "We can use Google Pre-built Kebeflow componets such as  EndpointCreateOp, ModelUploadOp, and ModelDeployOp to deploy the models locally at Vertex AI.\n",
    "\n",
    "***This is only for testing.  In your assigment, please use custom serving applications and CI-CD pipelines to deploy models. We should be able to deploy a given model at any given production environment. CI-CD pipelines are the best solution. ***\n",
    "\n",
    "<img src=\"imgs/EnterpriseMlOps_Model_Deployment.png\">\n",
    "\n",
    "source: https://cloud.redhat.com/blog/enterprise-mlops-reference-design\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the workflow of the pipeline.\n",
    "# @kfp.dsl.pipeline(\n",
    "#     name=\"hairloss-predictor-training-pipeline\")\n",
    "# def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str):    \n",
    "    \n",
    "#     di_op = download_data(\n",
    "#         project_id=project_id,\n",
    "#         bucket=data_bucket,\n",
    "#         file_name=trainset_filename\n",
    "#     )\n",
    "    \n",
    "#     train_test_split_op = train_test_split(dataset=di_op.output)\n",
    "        \n",
    "#     training_lr_job_run_op = train_lr(features=train_test_split_op.outputs[\"dataset_train\"])\n",
    "    \n",
    "#     model_evaluation_op = lr_model_evaluation(\n",
    "#         test_set=train_test_split_op.outputs[\"dataset_test\"],\n",
    "#         model_lr=training_lr_job_run_op.outputs[\"model\"],\n",
    "#         thresholds_dict_str=thresholds_dict_str, # I deploy the model anly if the model performance is above the threshold\n",
    "#     )\n",
    "    \n",
    "#     with dsl.If(\n",
    "#         model_evaluation_op.outputs[\"approval\"]== True,\n",
    "#         name=\"approve-model\",\n",
    "#     ):\n",
    "#         upload_model_to_gc_op = upload_model_to_gcs(\n",
    "#             project_id=project_id,\n",
    "#             model_repo=model_repo,\n",
    "#             model=training_lr_job_run_op.outputs['model']\n",
    "#         )    \n",
    "        \n",
    "#         import_unmanaged_model_task = dsl.importer(\n",
    "#             artifact_uri=model_repo_uri,\n",
    "#             artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "#             metadata={\n",
    "#                 \"containerSpec\": {\n",
    "#                     \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\",  # see https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers  \n",
    "#                 },\n",
    "#             },\n",
    "#         ).after(upload_model_to_gc_op)      \n",
    "       \n",
    "#         # using Google's custom components for for uloading and deploying the model.\n",
    "       \n",
    "#         model_upload_op = ModelUploadOp(\n",
    "#             project=project_id,\n",
    "#             display_name=\"hairloss-prediction-model\",\n",
    "#             unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "#         ).after(import_unmanaged_model_task)       \n",
    "               \n",
    "#         create_endpoint_op = EndpointCreateOp(\n",
    "#             project=project_id,\n",
    "#             display_name=\"hairloss-prediction-service\",\n",
    "#         ).after(model_upload_op)      \n",
    "        \n",
    "#         model_deploy_op = ModelDeployOp(\n",
    "#             model=model_upload_op.outputs[\"model\"],\n",
    "#             endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "#             deployed_model_display_name=\"hairloss-prediction-model\",\n",
    "#             dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "#             dedicated_resources_min_replica_count=1,\n",
    "#             dedicated_resources_max_replica_count=1,\n",
    "#             traffic_split={\"0\": 100},\n",
    "#         ).after(create_endpoint_op)   \n",
    "\n",
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"hairloss-predictor-training-pipeline\")\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str, trigger_id: str):   \n",
    "    \n",
    "    # Step 1: Download training data\n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "    training_dt = train_dt(\n",
    "        features=di_op.outputs['dataset'])\n",
    "    \n",
    "    training_lr = train_lr(\n",
    "        features=di_op.outputs['dataset'])\n",
    "    \n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_dt, training_lr)\n",
    "    \n",
    "    comp_model__op = compare_model(dt_metrics=training_dt.outputs[\"metrics\"],\n",
    "                                       lr_metrics=training_lr.outputs[\"metrics\"]).after(training_dt, training_lr)  \n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output==\"DT\"):\n",
    "        predict_dt_job_run_op = predict_dt(\n",
    "            model=training_dt.outputs[\"out_model\"],      \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_dt_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_dt.outputs['out_model']\n",
    "        ).after(predict_dt_job_run_op)\n",
    "        \n",
    "        trigger_model_deployment_cicd = run_build_trigger(\n",
    "            project_id=project_id,\n",
    "            trigger_id=trigger_id\n",
    "        ).after(upload_model_dt_to_gc_op)  \n",
    "        \n",
    "    with dsl.If(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "            model=training_lr.outputs[\"out_model\"],     \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_lr_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr.outputs['out_model']\n",
    "        ).after(predict_lr_job_run_op) \n",
    "        \n",
    "        trigger_model_deployment_cicd = run_build_trigger(\n",
    "            project_id=project_id,\n",
    "            trigger_id=trigger_id\n",
    "        ).after(upload_model_lr_to_gc_op) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='hairloss_predictor_training_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3a9d11a-e1e6-470d-8a9a-02d3e22c34de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/hairloss-predictor-training-pipeline-20241029121021?project=136177505402\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/136177505402/locations/europe-west1/pipelineJobs/hairloss-predictor-training-pipeline-20241029121021\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"hairloss-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"hairloss_predictor_training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': 'endless-mile-435507-h9', # makesure to use your project id \n",
    "        'data_bucket': 'hairloss-de-data',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'training_set.csv',     # makesure to upload these to your data bucket from DE2024/lab4/data\n",
    "        'testset_filename': 'test_set.csv',    # makesure to upload these to your data bucket from DE2024/lab4/data\n",
    "        'model_repo':'hairloss-de-models', # makesure to use your model bucket name \n",
    "        'trigger_id':'5fd4d88c-0e8d-4b6b-a093-b19d2f1e6eb4'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0ecf4-2179-4896-8582-5e565eb40c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9069e-d4dd-4857-ac2e-4db9a0470275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
